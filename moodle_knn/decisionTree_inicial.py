# -*- coding: utf-8 -*-
"""decisionTree-inicial.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/SirWillian/PatternRecognition/blob/master/moodle_knn/decisionTree-inicial.ipynb
"""

import os
import numpy as np
import matplotlib.pylab as plt

import sys
#!{sys.executable} -m pip install pydotplus
#!conda install --yes --prefix {sys.prefix} graphviz
import pydotplus 

import sklearn.metrics
from sklearn import tree
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import NearestNeighbors
from sklearn.datasets import load_iris

from sklearn.model_selection import train_test_split
from sklearn.model_selection import RandomizedSearchCV
from sklearn.model_selection import cross_val_score

#from IPython.display import Image
#from scipy.stats import randint

#carrega a base iris do próprio sckit-learn
iris = load_iris()

# carregando todas as características em X e os rótulos (labels, targets) em y
X = iris.data
y = iris.target #Sepal Length, Sepal Width, Petal Length and Petal Width.

print("As características são: " , iris.feature_names)

#separando em conjunto de treinamento e teste
X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size = 0.4)

# imprime as dimensões das bases
print("Dimensões das bases de treinamento e teste:")
print(X_train.shape)
print(X_test.shape)
print(y_train.shape)
print(y_test.shape)

#cria o classificador
clf = tree.DecisionTreeClassifier(criterion='gini', max_depth=2) #testar com (criterion='entropy')

#treina o classificador com a base de treinamento
clf = clf.fit(X_train, y_train)

#testa o classificador com a base de testes
preditor = clf.predict(X_test)

##########################
#Exercício 02 - A
#mostrar a acurácia
##########################
acc = sklearn.metrics.accuracy_score(y_test, preditor)
print("\nAccuracy\n", acc)

##########################
#Exercício 02 - B
##Analise a árvore de decisão criada
##########################
#cria uma imagem para mostrar a árvore de decisão criada
plt.figure(figsize=(10,10))
tree.plot_tree(clf, feature_names=iris.feature_names, class_names=iris.target_names);

from sklearn.neighbors import KNeighborsClassifier
neigh = KNeighborsClassifier(n_neighbors=7)
neigh.fit(X_train, y_train)

knn_predict = neigh.predict(X_test)
print("KNN Accuracy: ", sklearn.metrics.accuracy_score(y_test, knn_predict))
# try this later: http://ogrisel.github.io/scikit-learn.org/sklearn-tutorial/auto_examples/tutorial/plot_knn_iris.html

train_test_accuracies = {}
cross_val_accuracies = {}

for k in range(1,90):
    tt_clf_accuracy = []
    knn_clf = KNeighborsClassifier(n_neighbors=k)
    for i in range(10):
        Xtrain, Xtest, ytrain, ytest = train_test_split(iris.data, iris.target, test_size = 0.4, random_state=i)
        knn_clf.fit(Xtrain, ytrain)
        knn_prediction = knn_clf.predict(Xtest)
        knn_acc = sklearn.metrics.accuracy_score(ytest, knn_prediction)
        tt_clf_accuracy.append(knn_acc)
    train_test_accuracies[k]=np.mean(tt_clf_accuracy)
    cv_scores = cross_val_score(knn_clf, iris.data,iris.target,cv=5)
    cross_val_accuracies[k]=np.mean(cv_scores)
    
plt.plot(train_test_accuracies.keys(), train_test_accuracies.values(), label='Train test split')
plt.plot(cross_val_accuracies.keys(), cross_val_accuracies.values(), label='Cross validation')
plt.show()

test_train_best_ks = [key  for (key, value) in train_test_accuracies.items() if value == max(train_test_accuracies.values())]
cv_best_ks = [key  for (key, value) in cross_val_accuracies.items() if value == max(cross_val_accuracies.values())]
print("Test Train split best Ks:", test_train_best_ks)
print("Cross Validation best Ks:", cv_best_ks)

from sklearn.naive_bayes import GaussianNB
gnb = GaussianNB()
gnb_pred = gnb.fit(X_train, y_train).predict(X_test)

def print_metrics(estimator_name, ground_truth, prediction):
    print(estimator_name, "metrics:")
    print("Accuracy:", sklearn.metrics.accuracy_score(ground_truth, prediction))
    print("Precision:", sklearn.metrics.precision_score(ground_truth, prediction, average='macro'))
    print("Recall:", sklearn.metrics.recall_score(ground_truth, prediction, average='macro'))
    print("F-Measure:", sklearn.metrics.f1_score(ground_truth, prediction, average='macro'))
    print("Confusion matrix: \n", sklearn.metrics.confusion_matrix(ground_truth, prediction), "\n")

print_metrics("Decision Tree", y_test, preditor)
print_metrics("Naive Bayes", y_test, gnb_pred)
print_metrics("KNN", y_test, knn_predict)

from sklearn.datasets import fetch_openml
MNIST_X, MNIST_y = fetch_openml('mnist_784', version=1, return_X_y=True)

MNIST_X_train, MNIST_X_test, MNIST_y_train, MNIST_y_test = train_test_split(MNIST_X, MNIST_y, train_size=0.1, test_size=0.05)
print(MNIST_X_train.shape, MNIST_y_train.shape)

#mnist_cross_val_accuracies = {}
#for k in range(1,8,2):
k=3
knn_clf = KNeighborsClassifier(n_neighbors=k)
mnist_cv_scores = cross_val_score(knn_clf, MNIST_X_train, MNIST_y_train, cv=3)
#mnist_cross_val_accuracies[k]=np.mean(mnist_cv_scores)
#mnist_best_ks = [key  for (key, value) in mnist_cross_val_accuracies.items() if value == max(mnist_cross_val_accuracies.values())]
#print(mnist_best_ks)
print("Cross validation score:", np.mean(mnist_cv_scores))

#plt.plot(mnist_cross_val_accuracies.keys(), mnist_cross_val_accuracies.values())
knn_clf.fit(MNIST_X_train, MNIST_y_train)
mnist_predict = knn_clf.predict(MNIST_X_test)
print_metrics("MNIST KNN", MNIST_y_test, mnist_predict)
#print("Accuracy:", sklearn.metrics.accuracy_score(MNIST_y_test, mnist_predict))
#print("Precision:", sklearn.metrics.precision_score(MNIST_y_test, mnist_predict, average='micro'))
#print("Recall:", sklearn.metrics.recall_score(MNIST_y_test, mnist_predict, average='micro'))
#print("F-Measure:", sklearn.metrics.f1_score(MNIST_y_test, mnist_predict, average='micro'))
#print("Confusion matrix: \n", sklearn.metrics.confusion_matrix(MNIST_y_test, mnist_predict))